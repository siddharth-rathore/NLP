{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d823270",
   "metadata": {},
   "source": [
    "# AI-Driven News Sentiment & Weekly Summarization for Stock Movement (NASDAQ)\n",
    "**Created:** 2025-08-28\n",
    "\n",
    "> **Important:** In Colab, go to **Runtime â†’ Change runtime type â†’ Hardware accelerator = GPU** (T4 or \"GPU\"). Then **Save**.\n",
    ">\n",
    "> **Submission:** Export this notebook to **HTML** (File â†’ Download â†’ Download as .ipynb, then convert to HTML) or use Colab's *Print* to PDF/HTML.\n",
    "\n",
    "**What you'll do in this notebook**\n",
    "1. Load daily news + OHLCV for a single NASDAQ-listed company.\n",
    "2. EDA (univariate/bivariate) with clear plots and insights.\n",
    "3. Preprocess text & create train/validation/test splits (time-aware).\n",
    "4. Build 3 embedding pipelines:\n",
    "   - Word2Vec (train on corpus)\n",
    "   - GloVe (pretrained)\n",
    "   - Sentence-Transformer (MiniLM)\n",
    "5. Train a classifier for each embedding (with basic hyperparameter search).\n",
    "6. Evaluate with appropriate metrics & pick the best model; test-set results.\n",
    "7. Weekly news summarization using an open-source LLM from Hugging Face.\n",
    "8. Actionable insights & recommendations.\n",
    "\n",
    "> **Data format required (CSV):**\n",
    ">\n",
    "> - `Date` (YYYY-MM-DD)\n",
    "> - `News` (string)\n",
    "> - `Open`, `High`, `Low`, `Close` (float)\n",
    "> - `Volume` (int)\n",
    "> - `Label` in \\{-1, 0, 1\\} (Negative, Neutral, Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd28b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Environment Setup\n",
    "Run the cell below once in Colab. It installs the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, this will install/upgrade all deps.\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install numpy pandas scikit-learn matplotlib gensim nltk imbalanced-learn\n",
    "!pip -q install sentence-transformers transformers accelerate torch --extra-index-url https://download.pytorch.org/whl/cu121 || pip -q install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8472b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Imports, Config, and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d1b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Data Loading\n",
    "- Upload your CSV or mount Google Drive and set a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c39038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ”¼ Upload your dataset (CSV) here\n",
    "# In Colab, use the file uploader widget (uncomment next lines)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# CSV_FILE = list(uploaded.keys())[0]\n",
    "\n",
    "# OR set a path manually (e.g., from Drive)\n",
    "CSV_FILE = \"news_stock_sample.csv\"  # Change this to your filename\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Basic checks\n",
    "expected_cols = {'Date','News','Open','High','Low','Close','Volume','Label'}\n",
    "missing = expected_cols - set(df.columns)\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Parse dates\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c99f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Problem Definition\n",
    "**Goal:** Predict **news sentiment** (Label: -1, 0, 1) from text, and connect weekly summaries to price action.\n",
    "\n",
    "**Business Value:** Better sentiment nowcasting â†’ improved trading decisions, position sizing, and risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b6763",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Exploratory Data Analysis (EDA)\n",
    "### 4.1 Univariate\n",
    "- Daily price/volume distributions\n",
    "- News length distribution\n",
    "- Label class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ffcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats for numeric columns\n",
    "display(df[['Open','High','Low','Close','Volume']].describe())\n",
    "\n",
    "# Add aux features\n",
    "df['news_len'] = df['News'].astype(str).str.split().apply(len)\n",
    "\n",
    "# Plots\n",
    "df['news_len'].hist(bins=40); plt.title('News length (tokens)'); plt.xlabel('Tokens'); plt.ylabel('Count'); plt.show()\n",
    "\n",
    "df['Label'].value_counts().sort_index().plot(kind='bar'); plt.title('Label distribution (-1, 0, 1)'); plt.xlabel('Label'); plt.ylabel('Count'); plt.show()\n",
    "\n",
    "df.set_index('Date')['Close'].plot(); plt.title('Close over time'); plt.ylabel('Price'); plt.show()\n",
    "\n",
    "df.set_index('Date')['Volume'].plot(); plt.title('Volume over time'); plt.ylabel('Shares'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a3d65",
   "metadata": {},
   "source": [
    "### 4.2 Bivariate\n",
    "- Label vs price change\n",
    "- Label vs volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9fd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price change %\n",
    "df['pct_change'] = df['Close'].pct_change()*100\n",
    "\n",
    "# Group by label for insights\n",
    "label_grp = df.groupby('Label')['pct_change'].agg(['mean','median','count'])\n",
    "display(label_grp)\n",
    "\n",
    "# Boxplots: pct_change by label\n",
    "df.boxplot(column='pct_change', by='Label'); plt.title('Pct change by Sentiment Label'); plt.suptitle(''); plt.ylabel('%'); plt.show()\n",
    "\n",
    "# Correlations\n",
    "numerics = df[['Open','High','Low','Close','Volume','news_len']].corr()\n",
    "print(numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07cabd",
   "metadata": {},
   "source": [
    "> ðŸ“ **Key EDA Observations (fill in after running):**\n",
    "- Class balance & potential imbalance.\n",
    "- Relationship between sentiment and next-day returns (optional to add lag).\n",
    "- Any outliers or missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d85d82",
   "metadata": {},
   "source": [
    "---\n",
    "## 5) Preprocessing\n",
    "- Clean text (lowercase, remove URLs/punct, lemmatize).\n",
    "- Define X (text) and y (labels).\n",
    "- Time-aware split: train (60%), valid (20%), test (20%) in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(s:str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    tokens = [lemm.lemmatize(t) for t in s.split() if t not in stops and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean'] = df['News'].astype(str).apply(clean_text)\n",
    "\n",
    "X = df['clean'].values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Chronological split\n",
    "n = len(df)\n",
    "train_end = int(0.6*n)\n",
    "valid_end = int(0.8*n)\n",
    "\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_valid, y_valid = X[train_end:valid_end], y[train_end:valid_end]\n",
    "X_test,  y_test  = X[valid_end:], y[valid_end:]\n",
    "\n",
    "print(len(X_train), len(X_valid), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436a01f",
   "metadata": {},
   "source": [
    "---\n",
    "## 6) Word Embeddings\n",
    "We'll create three embedding strategies and related vectorizers:\n",
    "\n",
    "1. **Word2Vec (train on our corpus)** â†’ average word vectors per document  \n",
    "2. **GloVe (pretrained `glove-wiki-gigaword-100`)** â†’ average vectors  \n",
    "3. **Sentence-Transformer (`all-MiniLM-L6-v2`)** â†’ direct sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Word2Vec â€” train on our corpus\n",
    "tokenized = [s.split() for s in X_train]\n",
    "w2v = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=2, workers=4, seed=42, epochs=20)\n",
    "w2v_kv = w2v.wv\n",
    "\n",
    "def doc_embed_w2v(text, kv=w2v_kv, dim=100):\n",
    "    toks = text.split()\n",
    "    vecs = [kv[w] for w in toks if w in kv]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def embed_corpus_w2v(corpus):\n",
    "    return np.vstack([doc_embed_w2v(s) for s in corpus])\n",
    "\n",
    "Xtr_w2v = embed_corpus_w2v(X_train)\n",
    "Xva_w2v = embed_corpus_w2v(X_valid)\n",
    "Xte_w2v = embed_corpus_w2v(X_test)\n",
    "Xtr_w2v.shape, Xva_w2v.shape, Xte_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e52fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 GloVe â€” pretrained 100d\n",
    "glove = api.load('glove-wiki-gigaword-100')  # downloads once in Colab\n",
    "def doc_embed_glove(text, kv=glove, dim=100):\n",
    "    toks = text.split()\n",
    "    vecs = [kv[w] for w in toks if w in kv]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def embed_corpus_glove(corpus):\n",
    "    return np.vstack([doc_embed_glove(s) for s in corpus])\n",
    "\n",
    "Xtr_glove = embed_corpus_glove(X_train)\n",
    "Xva_glove = embed_corpus_glove(X_valid)\n",
    "Xte_glove = embed_corpus_glove(X_test)\n",
    "Xtr_glove.shape, Xva_glove.shape, Xte_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Sentence-Transformer â€” MiniLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "Xtr_st = st_model.encode(list(X_train), batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "Xva_st = st_model.encode(list(X_valid), batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "Xte_st = st_model.encode(list(X_test),  batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "Xtr_st.shape, Xva_st.shape, Xte_st.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2d106",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) Modeling & Hyperparameter Tuning\n",
    "We'll test two lightweight classifiers:\n",
    "- **Logistic Regression** (multinomial)\n",
    "- **Linear SVM (LinearSVC)**\n",
    "\n",
    "> **Primary metric:** **Macro F1** (balances performance across {-1,0,1} even if classes are imbalanced). We'll also report Accuracy and per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_and_print(y_true, y_pred, title=\"\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "    print(\"Macro F1:\", round(f1_score(y_true, y_pred, average='macro'), 4))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[-1,0,1])\n",
    "    print(\"\\nConfusion Matrix (rows=true, cols=pred, labels=[-1,0,1]):\\n\", cm)\n",
    "\n",
    "# Hyperparameter spaces\n",
    "logreg_params = {\n",
    "    'C': np.logspace(-2, 2, 10),\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': np.logspace(-2, 2, 10),\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def tune_and_eval(Xtr, ytr, Xva, yva, space, model_name=\"logreg\"):\n",
    "    if model_name == \"logreg\":\n",
    "        base = LogisticRegression(multi_class='auto', random_state=42)\n",
    "        search = RandomizedSearchCV(base, space, n_iter=12, scoring='f1_macro',\n",
    "                                    cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
    "    else:\n",
    "        base = LinearSVC(random_state=42)\n",
    "        search = RandomizedSearchCV(base, space, n_iter=12, scoring='f1_macro',\n",
    "                                    cv=3, random_state=42, n_jobs=-1, verbose=0)\n",
    "    search.fit(Xtr, ytr)\n",
    "    best = search.best_estimator_\n",
    "    yhat = best.predict(Xva)\n",
    "    evaluate_and_print(yva, yhat, title=f\"{model_name} (best on validation)\")\n",
    "    return best\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Word2Vec\n",
    "print(\"Tuning on Word2Vec embeddings...\")\n",
    "best_lr_w2v = tune_and_eval(Xtr_w2v, y_train, Xva_w2v, y_valid, logreg_params, \"logreg\")\n",
    "best_svm_w2v = tune_and_eval(Xtr_w2v, y_train, Xva_w2v, y_valid, svm_params, \"svm\")\n",
    "results['w2v_lr'] = best_lr_w2v\n",
    "results['w2v_svm'] = best_svm_w2v\n",
    "\n",
    "# GloVe\n",
    "print(\"\\nTuning on GloVe embeddings...\")\n",
    "best_lr_glove = tune_and_eval(Xtr_glove, y_train, Xva_glove, y_valid, logreg_params, \"logreg\")\n",
    "best_svm_glove = tune_and_eval(Xtr_glove, y_train, Xva_glove, y_valid, svm_params, \"svm\")\n",
    "results['glove_lr'] = best_lr_glove\n",
    "results['glove_svm'] = best_svm_glove\n",
    "\n",
    "# Sentence-Transformer\n",
    "print(\"\\nTuning on Sentence-Transformer embeddings...\")\n",
    "best_lr_st = tune_and_eval(Xtr_st, y_train, Xva_st, y_valid, logreg_params, \"logreg\")\n",
    "best_svm_st = tune_and_eval(Xtr_st, y_train, Xva_st, y_valid, svm_params, \"svm\")\n",
    "results['st_lr'] = best_lr_st\n",
    "results['st_svm'] = best_svm_st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd209ff",
   "metadata": {},
   "source": [
    "### 7.1 Model Selection\n",
    "Pick the **best validation Macro F1** model and evaluate it on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def macro_f1_on_valid(clf, Xva, yva):\n",
    "    yhat = clf.predict(Xva)\n",
    "    return f1_score(yva, yhat, average='macro')\n",
    "\n",
    "candidates = {\n",
    "    'w2v_lr': (results['w2v_lr'], Xva_w2v),\n",
    "    'w2v_svm': (results['w2v_svm'], Xva_w2v),\n",
    "    'glove_lr': (results['glove_lr'], Xva_glove),\n",
    "    'glove_svm': (results['glove_svm'], Xva_glove),\n",
    "    'st_lr': (results['st_lr'], Xva_st),\n",
    "    'st_svm': (results['st_svm'], Xva_st),\n",
    "}\n",
    "\n",
    "best_name, best_clf, best_feats = None, None, None\n",
    "best_score = -1.0\n",
    "for name, (clf, Xva_) in candidates.items():\n",
    "    score = macro_f1_on_valid(clf, Xva_, y_valid)\n",
    "    print(f\"{name}: val Macro F1 = {score:.4f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_name, best_clf, best_feats = name, clf, Xva_\n",
    "print(f\"\\nBest model on validation: {best_name} (Macro F1={best_score:.4f})\")\n",
    "\n",
    "# Test-set features based on winner\n",
    "if best_name.startswith('w2v'):\n",
    "    Xte = Xte_w2v\n",
    "elif best_name.startswith('glove'):\n",
    "    Xte = Xte_glove\n",
    "else:\n",
    "    Xte = Xte_st\n",
    "\n",
    "yhat_test = best_clf.predict(Xte)\n",
    "evaluate_and_print(y_test, yhat_test, title=f\"FINAL TEST â€” {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11ca0f",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Weekly News Summarization (LLM)\n",
    "**Task:** For each week, identify **Top 3 Positive** and **Top 3 Negative** events likely to impact price.\n",
    "\n",
    "We will:\n",
    "1. Group by ISO week (`Date`).\n",
    "2. Concatenate that week's news text.\n",
    "3. Use an instruction-tuned model (`google/flan-t5-base`) to produce structured bullets.\n",
    "4. Parse into a tidy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def load_llm(model_name='google/flan-t5-base', device=None):\n",
    "    if device is None:\n",
    "        device = 0 if (hasattr(torch, 'cuda') and torch.cuda.is_available()) else -1\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    gen = pipeline('text2text-generation', model=mdl, tokenizer=tok, device=device)\n",
    "    return gen\n",
    "\n",
    "def chunk_text(text, max_words=400):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield \" \".join(words[i:i+max_words])\n",
    "\n",
    "def summarize_week(gen, text):\n",
    "    partial_summaries = []\n",
    "    for chunk in chunk_text(text, max_words=350):\n",
    "        prompt = \"Summarize the following financial news in 3-5 concise bullet points focusing on stock price drivers:\\n\\n\" + chunk\n",
    "        out = gen(prompt, max_new_tokens=180, temperature=0.2, do_sample=False)[0]['generated_text']\n",
    "        partial_summaries.append(out)\n",
    "\n",
    "    combined = \"\\n\".join(partial_summaries)\n",
    "    instruction = (\n",
    "        \"From the summary below, list exactly 3 Positive and 3 Negative events that could move the stock. \"\n",
    "        \"Return in this strict template:\\n\"\n",
    "        \"POS:\\n1) ...\\n2) ...\\n3) ...\\nNEG:\\n1) ...\\n2) ...\\n3) ...\\n\\n\"\n",
    "        \"SUMMARY:\\n\" + combined\n",
    "    )\n",
    "    final = gen(instruction, max_new_tokens=220, temperature=0.1, do_sample=False)[0]['generated_text']\n",
    "    return final\n",
    "\n",
    "def parse_structured_events(text):\n",
    "    pos, neg = [], []\n",
    "    pos_block = re.search(r\"POS\\s*:\\s*(.*?)\\n\\s*NEG\\s*:\", text, flags=re.S|re.I)\n",
    "    neg_block = re.search(r\"NEG\\s*:\\s*(.*)\", text, flags=re.S|re.I)\n",
    "    if pos_block:\n",
    "        lines = re.findall(r\"\\d+\\)\\s*(.+)\", pos_block.group(1))\n",
    "        pos = [l.strip(\" -â€¢\") for l in lines][:3]\n",
    "    if neg_block:\n",
    "        lines = re.findall(r\"\\d+\\)\\s*(.+)\", neg_block.group(1))\n",
    "        neg = [l.strip(\" -â€¢\") for l in lines][:3]\n",
    "    while len(pos) < 3: pos.append(\"\")\n",
    "    while len(neg) < 3: neg.append(\"\")\n",
    "    return pos, neg\n",
    "\n",
    "def weekly_summaries(df):\n",
    "    gen = load_llm()\n",
    "    weekly = (df\n",
    "              .set_index('Date')\n",
    "              .groupby(pd.Grouper(freq='W-MON', label='right'))\n",
    "              .agg({'News': lambda x: \"\\n\".join(x.astype(str)),\n",
    "                    'Close': 'last', 'Volume': 'sum'})\n",
    "              .dropna(subset=['News']))\n",
    "    records = []\n",
    "    for week_end, row in weekly.iterrows():\n",
    "        text = row['News']\n",
    "        result = summarize_week(gen, text)\n",
    "        pos, neg = parse_structured_events(result)\n",
    "        records.append({\n",
    "            'week_end': week_end.date().isoformat(),\n",
    "            'pos_1': pos[0], 'pos_2': pos[1], 'pos_3': pos[2],\n",
    "            'neg_1': neg[0], 'neg_2': neg[1], 'neg_3': neg[2],\n",
    "            'close_last': row['Close'], 'volume_sum': row['Volume']\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "weekly_df = weekly_summaries(df)\n",
    "weekly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645cc9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 9) Actionable Insights & Recommendations\n",
    "Fill in after observing your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757b7ee",
   "metadata": {},
   "source": [
    "**Insights (examples to refine):**\n",
    "- **Sentence-Transformer** embeddings typically outperform averaged word vectors on short texts; if your validation **Macro F1** is highest for ST, prefer that.\n",
    "- Neutral class is often hardest; consider thresholding approaches or label smoothing.\n",
    "- Weekly summaries can highlight catalysts (earnings, product launches, regulatory news) â€” align with spikes in volume/returns.\n",
    "- Time-aware split avoids leakage; for production, use rolling retrains and walk-forward validation.\n",
    "\n",
    "**Recommendations:**\n",
    "1. **Productionize the best pipeline** (embedding + classifier) behind an inference API; track latency and drift.\n",
    "2. **Add event features** (earnings dates, macro releases) to improve price movement models.\n",
    "3. **Use weekly LLM summaries** in analyst notes and as features (e.g., sentiment counts) for forecasting.\n",
    "4. **Monitor class balance** over time; retrain with recent data and consider focal loss or class weights.\n",
    "5. **Backtest** trading rules using the sentiment signal (e.g., long when 7-day sentiment z-score > threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e14d8",
   "metadata": {},
   "source": [
    "---\n",
    "## 10) Save Artifacts\n",
    "Save weekly summaries and export model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weekly summaries\n",
    "weekly_path = \"weekly_summaries.csv\"\n",
    "df_weekly_out = weekly_df.copy()\n",
    "df_weekly_out.to_csv(weekly_path, index=False)\n",
    "print(\"Saved:\", weekly_path)\n",
    "\n",
    "# (Optional) Save the final classifier and any embedding assets\n",
    "import joblib\n",
    "joblib.dump(best_clf, f\"best_model_{best_name}.joblib\")\n",
    "print(\"Saved:\", f\"best_model_{best_name}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bde41",
   "metadata": {},
   "source": [
    "---\n",
    "## 11) Appendix: Tips & Extensions\n",
    "- Add **next-day** or **multi-day forward returns** labels to connect sentiment â†’ price.\n",
    "- Try **class weights** or **CalibratedClassifierCV** for better neutrality handling.\n",
    "- Replace Flan-T5 with **BART** summarization (`facebook/bart-large-cnn`) if preferred.\n",
    "- Use **wandb** or **mlflow** for experiment tracking.\n",
    "- Convert this notebook to **HTML** for submission."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
